{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1. Manually Adjust Pose and Expression of An Avatar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/yhding/Repo/Imp_NeuAvatar/\")\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='15' ## please change this accordingly\n",
    "from nha.util.render import create_intrinsics_matrix\n",
    "import torch\n",
    "from nha.models.nha_optimizer import NHAOptimizer\n",
    "from nha.util.general import dict_2_device\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ckpt = \"ckpts_and_data/nha/person_0004.ckpt\" # pretrained model\n",
    "# ckpt = \"default_dir/lightning_logs/version_2/checkpoints/last.ckpt\" # pretrained model\n",
    "\n",
    "tracking_results=\"ckpts_and_data/tracking/person_0004.npz\" # head tracking file\n",
    "# tracking_results=\"OUTPUT_PATH/tracking_0/tracked_flame_params.npz\" # head tracking file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yhding/miniconda3/envs/avatar/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:51: UserWarning: ModelCheckpoint(save_last=True, monitor=None) is a redundant configuration. You can save the last checkpoint with ModelCheckpoint(save_top_k=None, monitor=None).\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/yhding/miniconda3/envs/avatar/lib/python3.9/site-packages/pytorch3d/structures/meshes.py:1107: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  self._edges_packed = torch.stack([u // V, u % V], dim=1)\n"
     ]
    }
   ],
   "source": [
    "avatar = NHAOptimizer.load_from_checkpoint(ckpt).eval().cuda()\n",
    "tr = np.load(tracking_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthesizing novel poses and expressions\n",
    "@torch.no_grad()\n",
    "def synthesize_novel_poses_and_expressions(expr = torch.zeros(100, dtype=torch.float),\n",
    "                                           pose = torch.zeros(15, dtype=torch.float), image_size = (512, 512)):\n",
    "    \n",
    "    img_h, img_w = image_size\n",
    "    track_h, track_w = tr['image_size']\n",
    "    fx_scale = max(track_h, track_w) * img_w / track_w\n",
    "    fy_scale = max(track_h, track_w) * img_h / track_h\n",
    "    cx_scale = img_w\n",
    "    cy_scale = img_h\n",
    "    cam_intrinsics = create_intrinsics_matrix(\n",
    "        fx=tr[\"K\"][0] * fx_scale,\n",
    "        fy=tr[\"K\"][0] * fy_scale,\n",
    "        px=tr[\"K\"][1] * cx_scale,\n",
    "        py=tr[\"K\"][2] * cy_scale,\n",
    "    ) \n",
    "\n",
    "    # creating batch with inputs to avatar\n",
    "    rest_joint_rots = avatar._flame.get_neutral_joint_rotations()\n",
    "    default_pose = torch.cat((rest_joint_rots[\"global\"], \n",
    "                              rest_joint_rots[\"neck\"], \n",
    "                              rest_joint_rots[\"jaw\"], \n",
    "                              rest_joint_rots[\"eyes\"],\n",
    "                              rest_joint_rots[\"eyes\"]\n",
    "                             ), dim=0).cpu()\n",
    "    \n",
    "    batch = dict(\n",
    "                flame_shape = torch.from_numpy(tr[\"shape\"][None]).float(),\n",
    "                flame_expr = expr[None],\n",
    "                flame_pose = (pose+default_pose)[None],\n",
    "                flame_trans = torch.from_numpy(tr[\"translation\"][[0]]).float(),\n",
    "                cam_intrinsic=cam_intrinsics[None],\n",
    "                cam_extrinsic=torch.from_numpy(tr[\"RT\"]).float()[None],\n",
    "                rgb=torch.zeros(1,3,img_h,img_w))    \n",
    "    \n",
    "    batch = dict_2_device(batch, avatar.device)\n",
    "    \n",
    "    \n",
    "    # make prediction\n",
    "    rgba = avatar.forward(batch, symmetric_rgb_range=False)\n",
    "    shaded_mesh = avatar.predict_shaded_mesh(batch)\n",
    "\n",
    "    return rgba, shaded_mesh\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdf44c888894b028db43a235ea6dfcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.5, description='jaw', max=1.0), FloatSlider(value=0.0, description='…"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adjust pose:\n",
    "    rot0,1,2 : global head rotation\n",
    "    neck0,1,2 : neck rotation\n",
    "\"\"\"\n",
    "\n",
    "from ipywidgets import interactive\n",
    "\n",
    "def f_pos(jaw, rot0, rot1, rot2, neck0, neck1, neck2):\n",
    "    expr = torch.zeros(100, dtype=torch.float)\n",
    "    pose = torch.zeros(15, dtype=torch.float)\n",
    "\n",
    "    # expression parameters\n",
    "    e0 = 0\n",
    "    e1 = 0\n",
    "    e2 = 0\n",
    "    e3 = 0\n",
    "    e4 = 0\n",
    "\n",
    "    expr[0] = e0; expr[1] = e1; expr[2] = e2; expr[3] = e3; expr[4] = e4\n",
    "    pose[0] = rot0; pose[1] = rot1;  pose[2] = rot2; pose[3] = neck0;  pose[4] = neck1; pose[5] = neck2; pose[6] = jaw\n",
    "    %time\n",
    "\n",
    "    rgba, shaded_mesh = synthesize_novel_poses_and_expressions(expr=expr, pose=pose, image_size=(512,512))\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(20,10))\n",
    "    axes[0].imshow(rgba[0,:3].cpu().permute(1,2,0))\n",
    "    axes[1].imshow(shaded_mesh[0, :3].cpu().permute(1,2,0))\n",
    "\n",
    "    [a.axis(\"off\") for a in axes]\n",
    "    plt.show()\n",
    "    \n",
    "interactive_plot_pos = interactive(f_pos, jaw = (.0, 1.0), rot0=(-.5, .5), rot1=(np.pi-1, np.pi+1),rot2=(-.5, .5), neck0=(-.5, .5), neck1=(-45,45), neck2=(-0.5,.5))\n",
    "# output = interactive_plot.children[-1]\n",
    "# output.layout.height = '350px'\n",
    "interactive_plot_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcec849acec042558e4bdc95f819f3fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.0, description='e0', max=5.0, min=-5.0, step=0.5), FloatSlider(value…"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Adjust expressions\n",
    "\"\"\"\n",
    "\n",
    "def f_exp(e0, e1, e2, e3, e4):\n",
    "    expr = torch.zeros(100, dtype=torch.float)\n",
    "    pose = torch.zeros(15, dtype=torch.float)\n",
    "\n",
    "    # global rotation\n",
    "    rot0 = 0\n",
    "    rot1 = np.pi\n",
    "    rot2 = 0\n",
    "\n",
    "    # neck rotation\n",
    "    neck0 = 0 # up and down (-.5, .5)\n",
    "    neck1 = 0 # twist (-45,45)\n",
    "    neck2 = 0 # tilte head (-.5, .5)\n",
    "\n",
    "    # jaw movement \n",
    "    jaw = 0 # (0,1)\n",
    "\n",
    "    expr[0] = e0; expr[1] = e1; expr[2] = e2; expr[3] = e3; expr[4] = e4\n",
    "    pose[0] = rot0; pose[1] = rot1;  pose[2] = rot2; pose[3] = neck0;  pose[4] = neck1; pose[5] = neck2; pose[6] = jaw\n",
    "    %time\n",
    "\n",
    "    rgba, shaded_mesh = synthesize_novel_poses_and_expressions(expr=expr, pose=pose, image_size=(512,512))\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(20,10))\n",
    "    axes[0].imshow(rgba[0,:3].cpu().permute(1,2,0))\n",
    "    axes[1].imshow(shaded_mesh[0, :3].cpu().permute(1,2,0))\n",
    "\n",
    "    [a.axis(\"off\") for a in axes]\n",
    "    plt.show()\n",
    "    \n",
    "interactive_plot_exp = interactive(f_exp, e0=(-5.0, 5.0,0.5), e1=(-5.0,5.0), e2=(-5.0,5.0), e3=(-5.0,5.0, 0.5), e4=(-5.0,5.0,1.0))\n",
    "interactive_plot_exp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
